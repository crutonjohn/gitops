{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"crutonjohn's gitops docs # <---- use the sidebar to check out some docs or don't. it's your life...whatever","title":"Home"},{"location":"#crutonjohns-gitops-docs","text":"<---- use the sidebar to check out some docs or don't. it's your life...whatever","title":"crutonjohn's gitops docs"},{"location":"about/","text":"","title":"About"},{"location":"dr/","text":"Disaster Recovery # Currently my DR plan is backed by kasten k10 , but I am also running velero just because. This doc is respectfully borrowed from Toboshii's Restore Process , and slightly adapted. Flux # First, create the flux-system namespace: kubectl create namespace flux-system --dry-run=client -o yaml | kubectl apply -f - Create the flux gpg secret: AWS_PROFILE=personal sops -d sops-secret.enc.yaml | kubectl apply -f - Create the flux deployment: kubectl apply --kustomize=./clusters/env/production/flux-system K10 PVC Restore # Create secret by using the DR token provided by your previously working K10 installation. Mine is stored in my cluster-secrets.yaml sops -d clusters/env/production/config/cluster-secrets.yaml kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key=<passphrase> Verify that the K10 application is deployed with flux: kubectl get po -n kasten-io Verify that the backup storage location is deployed: kubectl get pvc k10-backup -n kasten-io Navigate to the webui and verify the backup location Install the DR restore job with a manual helm install: - helm install k10-restore kasten/k10restore --namespace=kasten-io \\ --set sourceClusterID=<source-clusterID> \\ --set profile.name=<location-profile-name> Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"Disaster Recovery"},{"location":"dr/#disaster-recovery","text":"Currently my DR plan is backed by kasten k10 , but I am also running velero just because. This doc is respectfully borrowed from Toboshii's Restore Process , and slightly adapted.","title":"Disaster Recovery"},{"location":"dr/#flux","text":"First, create the flux-system namespace: kubectl create namespace flux-system --dry-run=client -o yaml | kubectl apply -f - Create the flux gpg secret: AWS_PROFILE=personal sops -d sops-secret.enc.yaml | kubectl apply -f - Create the flux deployment: kubectl apply --kustomize=./clusters/env/production/flux-system","title":"Flux"},{"location":"dr/#k10-pvc-restore","text":"Create secret by using the DR token provided by your previously working K10 installation. Mine is stored in my cluster-secrets.yaml sops -d clusters/env/production/config/cluster-secrets.yaml kubectl create secret generic k10-dr-secret \\ --namespace kasten-io \\ --from-literal key=<passphrase> Verify that the K10 application is deployed with flux: kubectl get po -n kasten-io Verify that the backup storage location is deployed: kubectl get pvc k10-backup -n kasten-io Navigate to the webui and verify the backup location Install the DR restore job with a manual helm install: - helm install k10-restore kasten/k10restore --namespace=kasten-io \\ --set sourceClusterID=<source-clusterID> \\ --set profile.name=<location-profile-name> Upon completion of the DR Restore job, go to the Applications card, select Removed under the Filter by status drop-down menu. Click restore under the application and select a restore point to recover from.","title":"K10 PVC Restore"},{"location":"flux/","text":"Install Flux # SOPS # Make sure that your .sops.yml is valid. Pre-create the flux-system namespace: kubectl create namespace flux-system --dry-run=client -o yaml | kubectl apply -f - Apply the sops GPG key secret to the cluster: For me, I need to specify my AWS profile as this is encrypted with my AWS KMS key. AWS_PROFILE=personal sops -d sops-secret.enc.yaml | kubectl apply -f - Flux Bootstrap # Firstly we need to set our GITHUB_TOKEN variable. To do this one needs to be generated in Github. See these flux docs for more info. I typically set this for my session, but it can also be set solely for the command. This is for fish shell btw set -gx GITHUB_TOKEN \"ghp_......\" Then just bootstrap. This example is for bootstrapping a production cluster, but you can bootstrap different environments, implying that the code exists to do so. flux bootstrap github \\ --components=source-controller,kustomize-controller,helm-controller,notification-controller \\ --path=clusters/env/production \\ --version=latest \\ --owner=crutonjohn \\ --repository=gitops","title":"Install flux"},{"location":"flux/#install-flux","text":"","title":"Install Flux"},{"location":"flux/#sops","text":"Make sure that your .sops.yml is valid. Pre-create the flux-system namespace: kubectl create namespace flux-system --dry-run=client -o yaml | kubectl apply -f - Apply the sops GPG key secret to the cluster: For me, I need to specify my AWS profile as this is encrypted with my AWS KMS key. AWS_PROFILE=personal sops -d sops-secret.enc.yaml | kubectl apply -f -","title":"SOPS"},{"location":"flux/#flux-bootstrap","text":"Firstly we need to set our GITHUB_TOKEN variable. To do this one needs to be generated in Github. See these flux docs for more info. I typically set this for my session, but it can also be set solely for the command. This is for fish shell btw set -gx GITHUB_TOKEN \"ghp_......\" Then just bootstrap. This example is for bootstrapping a production cluster, but you can bootstrap different environments, implying that the code exists to do so. flux bootstrap github \\ --components=source-controller,kustomize-controller,helm-controller,notification-controller \\ --path=clusters/env/production \\ --version=latest \\ --owner=crutonjohn \\ --repository=gitops","title":"Flux Bootstrap"},{"location":"k3s/","text":"Install k3s # provision/vars/ # inventory.yaml # Update inventory to reflect your target hosts mind the ansible_user variable Make sure your ssh keys are installed to the target machines in inventory.yaml kube-vip.yaml # update the VIP for the control plane align interface name to be that of the control plane nodes others # review the files for any applicable edits Permit direnv to load env # direnv allow .envrc Activate the pipenv # This will install the required pip packages and create a virtual python environment. pipenv shell Overlaying the required repos # gilt overlay gilt will copy the required files from the k8s-at-home cluster template and use ansible-galaxy to install the required ansible roles. this will essentially create a rough-in \"mirror\" of the cluster template repo with some of our own mixins/customizations. Bootstrapping the k3s cluster # ansible-playbook -i provision/ansible/inventory/inventory.yaml provision/ansible/playbooks/k3s-install.yaml The playbook will run and dump a kubeconfig in provision/kubeconfig , which will automatically be sourced by our direnv . Now your cluster should be running. Verify cluster status # kubectl get nodes -o wide","title":"Install k3s"},{"location":"k3s/#install-k3s","text":"","title":"Install k3s"},{"location":"k3s/#provisionvars","text":"","title":"provision/vars/"},{"location":"k3s/#inventoryyaml","text":"Update inventory to reflect your target hosts mind the ansible_user variable Make sure your ssh keys are installed to the target machines in inventory.yaml","title":"inventory.yaml"},{"location":"k3s/#kube-vipyaml","text":"update the VIP for the control plane align interface name to be that of the control plane nodes","title":"kube-vip.yaml"},{"location":"k3s/#others","text":"review the files for any applicable edits","title":"others"},{"location":"k3s/#permit-direnv-to-load-env","text":"direnv allow .envrc","title":"Permit direnv to load env"},{"location":"k3s/#activate-the-pipenv","text":"This will install the required pip packages and create a virtual python environment. pipenv shell","title":"Activate the pipenv"},{"location":"k3s/#overlaying-the-required-repos","text":"gilt overlay gilt will copy the required files from the k8s-at-home cluster template and use ansible-galaxy to install the required ansible roles. this will essentially create a rough-in \"mirror\" of the cluster template repo with some of our own mixins/customizations.","title":"Overlaying the required repos"},{"location":"k3s/#bootstrapping-the-k3s-cluster","text":"ansible-playbook -i provision/ansible/inventory/inventory.yaml provision/ansible/playbooks/k3s-install.yaml The playbook will run and dump a kubeconfig in provision/kubeconfig , which will automatically be sourced by our direnv . Now your cluster should be running.","title":"Bootstrapping the k3s cluster"},{"location":"k3s/#verify-cluster-status","text":"kubectl get nodes -o wide","title":"Verify cluster status"},{"location":"prereqs/","text":"Prerequisites # OS # Mac # brew install \\ fluxcd/tap/flux \\ sops \\ kubectl \\ kustomize \\ direnv Linux # Coming soon... Windows # lol jk Python # pip3 install pipenv","title":"Prerequisites"},{"location":"prereqs/#prerequisites","text":"","title":"Prerequisites"},{"location":"prereqs/#os","text":"","title":"OS"},{"location":"prereqs/#mac","text":"brew install \\ fluxcd/tap/flux \\ sops \\ kubectl \\ kustomize \\ direnv","title":"Mac"},{"location":"prereqs/#linux","text":"Coming soon...","title":"Linux"},{"location":"prereqs/#windows","text":"lol jk","title":"Windows"},{"location":"prereqs/#python","text":"pip3 install pipenv","title":"Python"}]}